{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b008d44c",
   "metadata": {},
   "source": [
    "# `Pre-processing workflow`\n",
    "#### `and export all files for GNPS FBMN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d77bcb",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyopenms import *\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant path for interim files\n",
    "\n",
    "path= \"results/interim\"\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Feature Detection\n",
    "\n",
    "input_mzml_files = glob.glob('Example_data/*.mzML')\n",
    "\n",
    "# 1.1) Mass trace detection\n",
    "\n",
    "for filename in input_mzml_files:\n",
    "    print(\"Mass Trace Detection: \", filename)\n",
    "    exp = MSExperiment()\n",
    "    MzMLFile().load(filename, exp)\n",
    "    exp.sortSpectra(True)\n",
    "    mass_traces = []\n",
    "    mtd = MassTraceDetection()\n",
    "    mtd_par = mtd.getDefaults()\n",
    "    mtd_par.setValue(\"mass_error_ppm\", 10.0) # high-res instrument, orbitraps\n",
    "    mtd_par.setValue(\"noise_threshold_int\", 1.0e04) # data-dependent (usually works for orbitraps)\n",
    "    mtd.setParameters(mtd_par)\n",
    "    mtd.run(exp, mass_traces, 0)\n",
    "\n",
    "# 1.2) Elution peak detection\n",
    "    print(\"Elution Peak Detection: \", filename)\n",
    "    mass_traces_split = []\n",
    "    mass_traces_final = []\n",
    "    epd = ElutionPeakDetection()\n",
    "    epd_par = epd.getDefaults()\n",
    "    epd_par.setValue(\"width_filtering\", \"fixed\")\n",
    "    epd.setParameters(epd_par)\n",
    "    epd.detectPeaks(mass_traces, mass_traces_split)\n",
    "     \n",
    "    if (epd.getParameters().getValue(\"width_filtering\") == \"auto\"):\n",
    "          epd.filterByPeakWidth(mass_traces_split, mass_traces_final)\n",
    "    else:\n",
    "          mass_traces_final = mass_traces_split\n",
    "\n",
    "# 1.3) Feature detection\n",
    "    print(\"Feature Detection: \", filename)\n",
    "    feature_map_FFM = FeatureMap()\n",
    "    feat_chrom = []\n",
    "    ffm = FeatureFindingMetabo()\n",
    "    ffm_par = ffm.getDefaults() \n",
    "    ffm_par.setValue(\"isotope_filtering_model\", \"none\")\n",
    "    ffm_par.setValue(\"remove_single_traces\", \"true\")\n",
    "    ffm_par.setValue(\"mz_scoring_by_elements\", \"false\")\n",
    "    ffm_par.setValue(\"report_convex_hulls\", \"true\")\n",
    "    ffm.setParameters(ffm_par)\n",
    "    ffm.run(mass_traces_final, feature_map_FFM, feat_chrom)\n",
    "    feature_map_FFM.setUniqueIds()\n",
    "    feature_map_FFM.setPrimaryMSRunPath([filename.encode()])\n",
    "    print(filename[7:-5] + \".featureXML\")\n",
    "    FeatureXMLFile().store(os.path.join(path, os.path.basename(filename)[:-5] + \".featureXML\"), feature_map_FFM)\n",
    "    \n",
    "print(\"Finished Feature Detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature files \n",
    "\n",
    "input_feature_files = sorted(glob.glob('results/interim/*.featureXML'))\n",
    "\n",
    "feature_maps = []\n",
    "for featurexml_file in input_feature_files:\n",
    "    fmap = FeatureMap()\n",
    "    FeatureXMLFile().load(featurexml_file, fmap)\n",
    "    feature_maps.append(fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd8231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Map alignment \n",
    "\n",
    "#use as reference for alignment, the file with the largest number of features (works well if you have a pooled QC for example)\n",
    "ref_index = [i[0] for i in sorted(enumerate([fm.size() for fm in feature_maps]), key=lambda x:x[1])][-1]\n",
    "\n",
    "aligner = MapAlignmentAlgorithmPoseClustering()\n",
    "\n",
    "#parameter optimization\n",
    "aligner_par= aligner.getDefaults()\n",
    "aligner_par.setValue(\"max_num_peaks_considered\", -1) #infinite\n",
    "aligner_par.setValue(\"superimposer:mz_pair_max_distance\", 0.05) \n",
    "aligner_par.setValue(\"pairfinder:distance_MZ:max_difference\", 10.0) #Never pair features with larger m/z distance\n",
    "aligner_par.setValue(\"pairfinder:distance_MZ:unit\", \"ppm\")\n",
    "aligner.setParameters(aligner_par)\n",
    "aligner.setReference(feature_maps[ref_index])\n",
    "\n",
    "for feature_map in feature_maps[:ref_index] + feature_maps[ref_index+1:]:\n",
    "    trafo = TransformationDescription() #save the transformed data points\n",
    "    aligner.align(feature_map, trafo)\n",
    "    transformer = MapAlignmentTransformer()\n",
    "    transformer.transformRetentionTimes(feature_map, trafo, True) \n",
    "\n",
    "#save the aligned feature maps\n",
    "for feature_map in feature_maps:    \n",
    "    feature_file = os.path.join(path, 'Aligned_' + os.path.basename(feature_map.getMetaValue('spectra_data')[0].decode())[:-5] +\".featureXML\")\n",
    "    FeatureXMLFile().store(feature_file, feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b37dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the features in a dataframe\n",
    "\n",
    "input_feature_files = sorted(glob.glob('results/interim/Aligned*.featureXML'))\n",
    "\n",
    "for filename in input_feature_files:\n",
    "    fmap = FeatureMap()\n",
    "    FeatureXMLFile().load(filename, fmap)\n",
    "    DF= fmap.get_df(export_peptide_identifications=False)\n",
    "    feature_csv= os.path.join(path, os.path.basename(filename)[8:-10] +\"csv\")\n",
    "    DF.to_csv(feature_csv)\n",
    "print(\"example:\", os.path.basename(filename))\n",
    "display(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea57bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Annotate features that have MS2 information\n",
    "\n",
    "use_centroid_rt= False\n",
    "use_centroid_mz= True\n",
    "protein_ids = []\n",
    "peptide_ids= []\n",
    "\n",
    "mapper = IDMapper()\n",
    "\n",
    "input_mzml_files= sorted(glob.glob(\"Example_data/*.mzML\"))\n",
    "\n",
    "for filename in input_mzml_files:\n",
    "    exp = MSExperiment()\n",
    "    MzMLFile().load(filename, exp)\n",
    "\n",
    "    for fmap in feature_maps:\n",
    "        peptide_ids = []\n",
    "        protein_ids = []\n",
    "        if os.path.basename(fmap.getMetaValue('spectra_data')[0].decode()) == os.path.basename(filename):\n",
    "            mapper.annotate(fmap, peptide_ids, protein_ids, use_centroid_rt, use_centroid_mz, exp)\n",
    "            featureidx_file = os.path.join(path, 'IDMapper_' + os.path.basename(filename[:-4]) +\"featureXML\")\n",
    "            FeatureXMLFile().store(featureidx_file, fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9779645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotated feature files \n",
    "\n",
    "input_feature_files = sorted(glob.glob('results/interim/IDMapper_*.featureXML'))\n",
    "\n",
    "feature_maps = []\n",
    "for featurexml_file in input_feature_files:\n",
    "    fmap = FeatureMap()\n",
    "    FeatureXMLFile().load(featurexml_file, fmap)\n",
    "    feature_maps.append(fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Feature grouping\n",
    "\n",
    "feature_grouper = FeatureGroupingAlgorithmKD()\n",
    "\n",
    "consensus_map = ConsensusMap()\n",
    "file_descriptions = consensus_map.getColumnHeaders()\n",
    "\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    file_description = file_descriptions.get(i, ColumnHeader())\n",
    "    file_description.filename = os.path.basename(feature_map.getMetaValue('spectra_data')[0].decode())\n",
    "    file_description.size = feature_map.size()\n",
    "    file_descriptions[i] = file_description\n",
    "\n",
    "feature_grouper.group(feature_maps, consensus_map)\n",
    "consensus_map.setColumnHeaders(file_descriptions)\n",
    "\n",
    "\n",
    "Consensus_file= os.path.join(path, 'consensus' + \".consensusXML\")\n",
    "ConsensusXMLFile().store(Consensus_file, consensus_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Filter out features that have not fragmented\n",
    "\n",
    "input_consensus= \"results/interim/consensus.consensusXML\"\n",
    "cmap = ConsensusMap()\n",
    "ConsensusXMLFile().load(input_consensus, cmap)\n",
    "new_map= ConsensusMap(cmap)\n",
    "new_map.clear(False)\n",
    "for f in cmap:\n",
    "    if f.getPeptideIdentifications() !=[]:\n",
    "        new_map.push_back(f)\n",
    "\n",
    "Consensus_file= os.path.join(path,'filtered' + \".consensusXML\")\n",
    "ConsensusXMLFile().store(Consensus_file, new_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8adc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intensities as a DataFrame\n",
    "intensities = new_map.get_intensity_df()\n",
    "\n",
    "# get meta data as DataFrame\n",
    "meta_data = new_map.get_metadata_df()\n",
    "\n",
    "# you can concatenate these two for a \"result\" DataFrame\n",
    "result = pd.concat([meta_data, intensities], axis=1)\n",
    "\n",
    "# if you don't need labeled index, remove it (and/or save with index = False)\n",
    "result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#change the format of the table to one that is compatible for GNPS FBMN:\n",
    "idx = 0\n",
    "new_col = \"CONSENSUS\"  # can be a list, a Series, an array or a scalar   \n",
    "result.insert(loc=idx, column='#CONSENSUS', value=new_col)   \n",
    "result= result.rename(columns= {\"charge\": \"charge_cf\", \"RT\": \"rt_cf\", \"mz\": \"mz_cf\", \"quality\": \"quality_cf\", \"width\": \"width_cf\"})\n",
    "result= result.drop([\"sequence\"], axis= 1)\n",
    "result= result.sort_index(axis=1)\n",
    "\n",
    "filemeta= new_map.getColumnHeaders()\n",
    "mapIDs = [k for k in filemeta.keys()]\n",
    "filename= []\n",
    "size=[]\n",
    "label= []\n",
    "for header in filemeta.values():\n",
    "    files= header.filename\n",
    "    sizes= header.size\n",
    "    labels= header.label\n",
    "    filename.append(files)\n",
    "    size.append(sizes)\n",
    "    label.append(labels)\n",
    "\n",
    "dict = {'id': mapIDs, 'filename': filename,'label': label,'size': size}\n",
    "DF= pd.DataFrame(dict)\n",
    "DF[\"id\"] = \"intensity_\"+ (DF[\"id\"]).astype(str)\n",
    "\n",
    "cols= result.columns\n",
    "for col in cols:\n",
    "    for i, path in enumerate(filename):\n",
    "        if path== col:\n",
    "            name= DF[\"id\"][i]\n",
    "            result.rename(columns={col: name}, inplace=True)\n",
    "\n",
    "cols = result.columns\n",
    "preordered = [\"#CONSENSUS\", \"charge_cf\", \"rt_cf\", \"mz_cf\", \"quality_cf\", \"width_cf\"]\n",
    "new_cols = preordered + [c for c in result.columns if c not in preordered]\n",
    "new_df = result.reindex(columns=new_cols)\n",
    "new_df.to_csv(\"results/interim/FeatureMatrix.csv\", sep=\"\\t\", index=None)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eff463",
   "metadata": {},
   "source": [
    "6) Create a metadata table for GNPS FBMN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd Example_data/ && ls *.mzML > filelist.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28444357",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list = [\"filename\"]\n",
    "metadata=pd.read_csv(\"Example_data/filelist.txt\", names=header_list, index_col= None)\n",
    "metadata['ATTRIBUTE_MAPID'] = np.arange(len(metadata))\n",
    "metadata[\"ATTRIBUTE_MAPID\"]= \"MAP\" + metadata[\"ATTRIBUTE_MAPID\"].astype(str)\n",
    "metadata['ATTRIBUTE_compound']=metadata['filename'].replace(\".mzML\", value=\"\", regex=True)\n",
    "metadata.to_csv(\"results/GNPSexport/metadata.tsv\", sep='\\t')\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd Example_data/ && rm filelist.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the map ID information with the feature quantification table\n",
    "\n",
    "metadata.filter([\"filename\", \"ATTRIBUTE_MAPID\"])\n",
    "metadata[\"#MAP\"]= (\"MAP\")\n",
    "metadata[\"id\"]= metadata[\"ATTRIBUTE_MAPID\"].str.extract(r\"(\\d)\")\n",
    "metadata= metadata.drop(columns=\"ATTRIBUTE_MAPID\")\n",
    "metadata= metadata[[\"#MAP\", \"id\", \"filename\"]]\n",
    "metadata.to_csv(\"results/interim/mapIDs.csv\", sep=\"\\t\", index=None)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_file= \"results/interim/mapIDs.csv\"\n",
    "feature_file= \"results/interim/FeatureMatrix.csv\"\n",
    "merged_file= \"results/GNPSexport/FeatureQuantificationTable.txt\"\n",
    "\n",
    "with open(map_file, \"r\") as map_file:\n",
    "    map_text = map_file.read()\n",
    "\n",
    "with open(feature_file, \"r\") as feature_file:\n",
    "    feature_text = feature_file.read()\n",
    "\n",
    "with open(merged_file, \"w\") as merged_file:\n",
    "    merged_file.write(map_text + feature_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b32c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all MS2 information in a .MGF file\n",
    "\n",
    "consensus= \"results/interim/filtered.consensusXML\"\n",
    "input_mzml_files=sorted(glob.glob(\"Example_data/*.mzML\"))\n",
    "out_file= \"results/GNPSexport/MSMS.mgf\"\n",
    "\n",
    "spectra_clustering= GNPSMGFFile()\n",
    "\n",
    "spectra_clustering.run(String(consensus),[s.encode() for s in input_mzml_files], String(out_file))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edde62aa2661007f0756e9790e7a328c288a583bf6ce768a355147dac67c8db8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
