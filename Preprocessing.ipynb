{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b008d44c",
   "metadata": {},
   "source": [
    "# `Pre-processing workflow`\n",
    "#### `and export all files for GNPS FBMN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d77bcb",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyopenms import *\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant path for interim files\n",
    "path = \"results/interim\"\n",
    "if not os.path.exists(path): # if it doesn't exist\n",
    "    os.mkdir(\"results\") # create a results directory\n",
    "    os.mkdir(path)  # create an interim directory for temporary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Feature Detection\n",
    "\n",
    "input_mzml_files = glob.glob('Example_data/*.mzML') # introduce a set of mzML files from the Example_data directory\n",
    "\n",
    "# 1.1) Mass trace detection\n",
    "\n",
    "for filename in input_mzml_files: # for each file in the set of files\n",
    "    print(\"Mass Trace Detection: \", filename) #print the filename\n",
    "    exp = MSExperiment()    \n",
    "    MzMLFile().load(filename, exp) # load each mzML file to an OpenMS file format (MSExperiment)\n",
    "\n",
    "    mass_traces = [] # introduce an empty list where the mass traces will be loaded\n",
    "    mtd = MassTraceDetection()\n",
    "    mtd_par = mtd.getDefaults() # get the default parameters in order to edit them\n",
    "    mtd_par.setValue(\"mass_error_ppm\", 10.0) # high-res instrument, orbitraps\n",
    "    mtd_par.setValue(\"noise_threshold_int\", 1.0e04) # data-dependent (usually works for orbitraps)\n",
    "    mtd.setParameters(mtd_par) # set the new parameters\n",
    "    mtd.run(exp, mass_traces, 0) # run mass trace detection\n",
    "\n",
    "# 1.2) Elution peak detection\n",
    "\n",
    "    print(\"Elution Peak Detection: \", filename)\n",
    "    mass_traces_deconvol = []\n",
    "    epd = ElutionPeakDetection()\n",
    "    epd_par = epd.getDefaults()\n",
    "    epd_par.setValue(\"width_filtering\", \"fixed\") # The fixed setting filters out mass traces outside the [min_fwhm: 1.0, max_fwhm: 60.0] interval\n",
    "    epd.setParameters(epd_par)\n",
    "    epd.detectPeaks(mass_traces, mass_traces_deconvol)\n",
    "     \n",
    "# 1.3) Feature detection\n",
    "\n",
    "    print(\"Feature Detection: \", filename)\n",
    "    feature_map_FFM = FeatureMap() # output features \n",
    "    chrom_out = [] # output chromatograms \n",
    "    ffm = FeatureFindingMetabo()\n",
    "    ffm_par = ffm.getDefaults() \n",
    "    ffm_par.setValue(\"remove_single_traces\", \"true\") # remove mass traces without satellite isotopic traces\n",
    "    ffm.setParameters(ffm_par)\n",
    "    ffm.run(mass_traces_deconvol, feature_map_FFM, chrom_out)\n",
    "    feature_map_FFM.setUniqueIds() # Assigns a new, valid unique id per feature\n",
    "    feature_map_FFM.setPrimaryMSRunPath([filename.encode()]) # Sets the file path to the primary MS run (usually the mzML file)\n",
    "    FeatureXMLFile().store(os.path.join(path, os.path.basename(filename)[:-5] + \".featureXML\"), feature_map_FFM)\n",
    "    \n",
    "print(\"Finished Feature Detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature files \n",
    "\n",
    "input_feature_files = glob.glob('results/interim/*.featureXML') # set of feature files\n",
    "\n",
    "feature_maps = [] # empty list to fill with FeatureMaps: the OpenMS file format for feature files\n",
    "for featurexml_file in input_feature_files:\n",
    "    fmap = FeatureMap()\n",
    "    FeatureXMLFile().load(featurexml_file, fmap) # load each file to a feature map\n",
    "    feature_maps.append(fmap) # append all maps to the empty list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd8231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Map alignment \n",
    "\n",
    "# use as reference for alignment, the file with the largest number of features (works well if you have a pooled QC for example)\n",
    "ref_index = feature_maps.index(sorted(feature_maps, key=lambda x: x.size())[-1])\n",
    "\n",
    "aligner = MapAlignmentAlgorithmPoseClustering()\n",
    "\n",
    "# parameter optimization\n",
    "aligner_par= aligner.getDefaults()\n",
    "aligner_par.setValue(\"max_num_peaks_considered\", -1) # infinite\n",
    "aligner_par.setValue(\"pairfinder:distance_MZ:max_difference\", 10.0) # Never pair features with larger m/z distance\n",
    "aligner_par.setValue(\"pairfinder:distance_MZ:unit\", \"ppm\")\n",
    "aligner.setParameters(aligner_par)\n",
    "aligner.setReference(feature_maps[ref_index])\n",
    "\n",
    "for feature_map in feature_maps[:ref_index] + feature_maps[ref_index+1:]:\n",
    "    trafo = TransformationDescription() # save the transformed data points\n",
    "    aligner.align(feature_map, trafo)\n",
    "    transformer = MapAlignmentTransformer()\n",
    "    transformer.transformRetentionTimes(feature_map, trafo, True) \n",
    "\n",
    "# save the aligned feature maps\n",
    "for feature_map in feature_maps:    \n",
    "    feature_file = os.path.join(path, 'Aligned_' + os.path.basename(feature_map.getMetaValue('spectra_data')[0].decode())[:-5] +\".featureXML\")\n",
    "    FeatureXMLFile().store(feature_file, feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea57bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) IDMapper annotate features that have MS2 information with peptide identifications which is unrelated. This step is important for FBMN\n",
    "# because we have to introduce only features that have fragmented. \n",
    "\n",
    "use_centroid_rt = False\n",
    "use_centroid_mz = True\n",
    "protein_ids = []\n",
    "peptide_ids = []\n",
    "\n",
    "mapper = IDMapper()\n",
    "\n",
    "input_mzml_files = glob.glob(\"Example_data/*.mzML\")\n",
    "\n",
    "for filename in input_mzml_files:\n",
    "    exp = MSExperiment()\n",
    "    MzMLFile().load(filename, exp)\n",
    "\n",
    "    for fmap in feature_maps:\n",
    "        peptide_ids = []\n",
    "        protein_ids = []\n",
    "        if os.path.basename(fmap.getMetaValue('spectra_data')[0].decode()) == os.path.basename(filename):\n",
    "            mapper.annotate(fmap, peptide_ids, protein_ids, use_centroid_rt, use_centroid_mz, exp)\n",
    "            featureidx_file = os.path.join(path, 'IDMapper_' + os.path.basename(filename[:-4]) +\"featureXML\")\n",
    "            FeatureXMLFile().store(featureidx_file, fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9779645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load annotated feature files \n",
    "\n",
    "input_feature_files = glob.glob('results/interim/IDMapper_*.featureXML')\n",
    "\n",
    "feature_maps = []\n",
    "for featurexml_file in input_feature_files:\n",
    "    fmap = FeatureMap()\n",
    "    FeatureXMLFile().load(featurexml_file, fmap)\n",
    "    feature_maps.append(fmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Feature grouping\n",
    "\n",
    "feature_grouper = FeatureGroupingAlgorithmKD()\n",
    "\n",
    "consensus_map = ConsensusMap()\n",
    "file_descriptions = consensus_map.getColumnHeaders()\n",
    "\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    file_description = file_descriptions.get(i, ColumnHeader())\n",
    "    file_description.filename = os.path.basename(feature_map.getMetaValue('spectra_data')[0].decode())\n",
    "    file_description.size = feature_map.size()\n",
    "    file_descriptions[i] = file_description\n",
    "\n",
    "feature_grouper.group(feature_maps, consensus_map)\n",
    "consensus_map.setUniqueIds()\n",
    "consensus_map.setColumnHeaders(file_descriptions)\n",
    "\n",
    "\n",
    "Consensus_file = os.path.join(path, 'consensus' + \".consensusXML\")\n",
    "ConsensusXMLFile().store(Consensus_file, consensus_map)\n",
    "\n",
    "df = consensus_map.get_df()\n",
    "df= df.drop(columns=\"sequence\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70395268",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df[df[\"quality\"] > 0.01], x=\"RT\", y=\"mz\", color=\"quality\")\n",
    "fig.update_layout(title=\"Consensus features\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Filter out features that have not fragmented\n",
    "\n",
    "input_consensus = \"results/interim/consensus.consensusXML\"\n",
    "cmap = ConsensusMap()\n",
    "ConsensusXMLFile().load(input_consensus, cmap)\n",
    "new_map = ConsensusMap(cmap)\n",
    "new_map.clear(False)\n",
    "for f in cmap:\n",
    "    if f.getPeptideIdentifications():\n",
    "        new_map.push_back(f)\n",
    "\n",
    "Consensus_file = os.path.join(path,'filtered' + \".consensusXML\")\n",
    "ConsensusXMLFile().store(Consensus_file, new_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6556bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all MS2 information in a .MGF file\n",
    "\n",
    "if not os.path.exists(\"results/GNPSexport\"): # if it doesn't exist\n",
    "    os.mkdir(\"results/GNPSexport\")  # make a new one\n",
    "\n",
    "consensus = \"results/interim/filtered.consensusXML\"\n",
    "input_mzml_files = glob.glob(\"Example_data/*.mzML\")\n",
    "out_file = \"results/GNPSexport/MSMS.mgf\"\n",
    "\n",
    "spectra_clustering = GNPSMGFFile()\n",
    "\n",
    "spectra_clustering.run(String(consensus),[s.encode() for s in input_mzml_files], String(out_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a .TXT table of features \n",
    "\n",
    "output_file = \"results/GNPSexport/FeatureQuantificationTable.txt\"\n",
    "IonIdentityMolecularNetworking.writeFeatureQuantificationTable(cmap, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28444357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metadata table from the list of mzML files compatible for GNPS\n",
    "\n",
    "metadata = pd.DataFrame()\n",
    "metadata[\"filename\"] = [file for file in os.listdir(\"Example_data\") if file.endswith(\".mzML\")]\n",
    "metadata[\"ATTRIBUTE_MAPID\"]= [\"MAP\" + str(i) for i in range(len(metadata))]\n",
    "metadata['ATTRIBUTE_compound'] = metadata['filename'].replace(\".mzML\", value=\"\", regex=True)\n",
    "metadata.to_csv(\"results/GNPSexport/metadata.tsv\", sep='\\t')\n",
    "\n",
    "metadata"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
